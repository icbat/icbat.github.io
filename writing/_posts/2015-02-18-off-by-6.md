---
layout: post
title: Off by 6 - the most subtle bug I've seen to date
---

I'm still a (very) young and naive developer, but this is the worst bug I've uncovered so far.

All examples below have been reconstructed from memory to try and communicate the gist of the idea. I've done my best to simplify them to the core of the point anyway.

## The problem:  E-905

This project coordinated a bunch of different disparate systems (seemingly) never intended to talk to one another to bill a customer. This meant that most issues were high-priority issues; if we don't fix them, the company can't get paid, which tends to be frowned upon. We had a particularly problematic sync job; truly legacy code. One 12,000 line "class" that used a slough of similarly monolithic static Utility classes (a topic for another post), no tests in sight. The kind of code you only know exists because it fails.

Its job was to take transactional data from a system called Constellation and update the corresponding records in system B (which my team owned). The original developers had, rather fittingly, called it C2B.

Somewhere in the pipeline, some of these transactions would get in to a failure state. We were greeted only with an error code "E-905:  Could not buy product [an actual product name]". Fantastically useful; that's what this job was doing, at a high level. So it failed to do the magic.

Another developer picked this up and ran with it while the rest of us fought other fires and tried to make some bug fixes. He came back solemn but hopeful:  our logs held no mention of E-905, so surely it was in Constellation and not C2B. We handed off to that team and went about our business, all the while our lone support person had to manually intervene every day.

I'd been on this team for 2-3 months when this issue first popped up. If my last group was a raging house fire, this new home was [London in 1666](https://en.wikipedia.org/wiki/Great_Fire_of_London). So the concepts of "production issue unsolved for a few weeks" and "workaround requires 8+ hours of manual intervention" was an uncomfortable familiarity at this point. We moved on and prayed to the prioritization gods; the Constellation team was always swamped with production outages, we'd have to get in line.

## The Daemon

The fire I was solving in the meantime was for a daemon processor in Constellation. Our team was the only one getting alerted that this processor was down, as some misguided setup led the primary monitor to page a wholly uninvolved team. Historically, my team had seen this in the past and set up their own monitor, not checking if the daemon was running but instead whether or not it was getting behind in its work. We were notified as the work queue slowly grew to an insurmountable backlog. I worked with one of the Constellation team members to find out what was wrong. It wasn't turned on, so I started it. Easy!

I sent a triumphant email to involved parties touting my glorious success. As usual, it wasn't that simple. Someone with historical knowledge finally piped up, letting us know very politely that I'd started it on the wrong server; there was another server out there with _copies_ that was just for this. Fine. Great. Where was it? That day, both Constellation and B learned of a server that only one person knew we all relied upon. Swell. They were even nice enough to stop my process on the wrong server.

My Constellation comrade and I went out and found this _new_ server, and the process was indeed running there! And the logs were completely empty. Great. We debated possible impacts while watching the daemon's work queue continue to grow. Finally, we decided there would be no harm in turning this daemon, while(true){} process on and off. Lo and behold, the queue shrank. All was well again. I still wasn't sure what it was for, but it was apparently vital.

## Steps to reproduce

As our queue of fires was doused one-by-one, I finally had time to investigate the E-905 errors. The verdict from our friends on Constellation was that the error stemmed from our system. They even found those errors in our logs! Maybe my teammate missed them, or maybe he didn't know where to look? At least I had a starting point that was further along than we'd had before.

I finally cracked open the C2B program in all its legacy. Ctrl-F'ing through the ruins, there were 7 places an E-905 could be raised, each for a completely unrelated condition. Turns out, this is a general class of error whose only action item was "call the developers". My favorite.

I made some time between the various other meetings and commitments to meet with our support person to figure out what they were seeing.

1. Failing transactions could be manually reprocessed inside of Constellation, which was standard procedure. This failed all the time, sometimes in interesting ways (E-905 -> some other error -> E-905 ->...)
2. Most if not all of these were cleared up in a nightly process that ran each transaction waiting and tried to reprocessed all failures.
3. If she used system B to reprocess each manual transaction, it cleared up as well.

Well that doesn't make any sense. We talked to as many folks with very vital tribal knowledge and learned something wholly terrifying:  the daemon I'd been helping fix with Constellation was point 1 in the above list.

I worked with our support person to watch the logs as she manually fixed one. While our logs are categorically unreadable, sometimes there's a useful nugget, and in this case I could drill down to a transaction and carefully apply `grep` to get at what it's doing. I tried to watch it fail; nothing. I tried to watch it succeed, and I could see it succeed. But wait, wasn't the daemon on a different box... what if there's an C2B over there as well? There would have to be, right? What crazy person would SSH to run a command like this? For once, we weren't doing something quite as insane as imaginable.

## Epiphany

{% gist icbat/c664fa1d4ac94bf83b54 %}

Huh, there were logs after all; they were just on a box that no one knew existed. Fine. That second message makes it look like we're sending bad data through to the system. I consulted an expert in the system, he immediately thought the negative dates were the problem. We ran some isolated tests and changing those to any non-negative date let that transaction go through. Where are those negatives coming from, though? The dates on the business object itself in Constellation's GUI seem fine; they're formatted the same as successful transactions and the numbers all seem sane. That's the kind of thing manual inspection often reveals; we're used to seeing users enter things incorrectly, with no care. That wasn't the case here, so where do those come from, anyway? Now we've got logs and line numbers, we found the following snippets:

{% gist icbat/ed53c7ac698ff714726b %}

For the uninformed, [Unix time](https://en.wikipedia.org/wiki/Unix_time) involves counting seconds from an arbitrary point in time. The Unix epoch is 1 Jan, 1970 at Midnight UTC/GMT. So why does our program's getZeroDate() return a date 6 hours, or rather 21600 seconds, before the epoch? And if this is returning a constant date, wouldn't this give me a time 21600 seconds before 0 each time it runs? How has this ever worked?

A six hour difference is a very big red flag where I'm from, though. This team struggles with this all the time. One of our systems is in UTC, the other is in local time, which is UTC-6. _Well, butts_. We ran a quick `date` command on each system (BS and Node1), and they were indeed in different time zones.

## Conclusion

Now that we knew what was wrong, I asked around and found someone had noticed a change in the server's timezone setting around the time we started seeing problems. They tried to think of any potential impacts and couldn't, so no one bothered to correct. Changing it back to the right timezone fixed our problem immediately, and we've made sure to make our getZeroDate() method return a more universal zero date (namely, 0 seconds regardless of the Calendar or local settings).
