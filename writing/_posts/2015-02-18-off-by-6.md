---
layout: post
title: Off by 6 - the most subtle bug I've seen to date
---

I'm still a (very) young and naive developer, but this is the worst bug I've uncovered so far.

All examples below have been reconstructed from memory to try and communicate the gist of the idea. I've done my best to simplify them to the core of the point anyway.

## The problem:  E-905

This project coordinated a bunch of different disparate systems (seemingly) never intended to talk to one another to bill a customer. This meant that most issues were high-priority issues; if we don't fix them, the company can't get paid, which tends to be frowned upon. We had a particularly problematic sync job; truly legacy code. One 12,000 line "class" that used a slough of similarly monolithic static Utility classes (a topic for another post), no tests in sight. The kind of code you only know exists because it fails.

Its job was to take transactional data from system A and update the corresponding records in system B (we owned B). Being the creative developers they Originators were, it was named A2B.

Somewhere in the pipeline, some of these transactions would get in to a failure state. We were greeted only with an error code "E-905:  Could not buy product [an actual product name]". Fantastically useful; that's what this job was doing, at a high level. So it failed to do the magic.

Another developer picked this up and ran with it while the rest of us fought other fires and tried to make some bug fixes. He came back solemn but hopeful:  our logs held no mention of E-905, so surely it was in A and not A2B. We handed off to that team and went about our business, all the while our lone support person had to manually intervene every day.

I'd been on this team for 2-3 months when this issue first popped up. If my last group was a raging house fire, this new home was [London in 1666](https://en.wikipedia.org/wiki/Great_Fire_of_London). So the concepts of "production issue unsolved for a few weeks" and "workaround requires 8+ hours of manual intervention" was an uncomfortable familiarity at this point. We moved on and prayed to the prioritization gods; the A team was always swamped with production outages, we'd have to get in line.

## The Daemon

The fire I was solving in the meantime was for a daemon processor in system A. Our team was the only one getting alerted that this processor was down, as some misguided setup led the primary monitor to page a wholly uninvovled team. Historically, my team had seen this in the past and set up their own monitor, not checking if the daemon was running but instead whether or not it was getting behind in its work. We were notified as the work queue slowly grew to an insurmountable backlog. I worked with one of the A-team members to find out what was wrong. It was stopped! So I started it. Easy!

I sent a triumphant email to involved parties touting my glorious success. As usual, it wasn't that simple. Someone with historical knowledge finally piped up, letting us know very politely that I'd started it on the wrong server; there was another server out there with _copies_ that was just for this. Fine. Great. Where was it? That day, teams A and B learned of a server that only one person knew we all relied upon. Swell. They were even nice enough to stop my process on the wrong server.

My A-team compatriot and I went out and found this _new_ server, and the process was indeed running there! And the logs were completely empty. Great. We debated impacts while watching the daemon's work queue continue to grow. Finally, we decided there would be no harm in turning this daemon, while(true) process on and off. Lo and behold, the queue shrank. All was well again.

## Steps to reproduce

- picking back up from cold trail
- 7 types of 905 error
- talk to deb, make list of exactly all the cases
- start to draw the picture for exactly what our process is
- now we know RAP does v2b too

## Logs!

- There's a seccond A2B! who'd have thought?
- there are logs on this box
- this is where Matt was telling us to look!
- Negative dates from the log!

## Epiphany

So we're sending bad data through to the other system. Where are those coming from, though? The dates on the business object itself seem fine; they're formatted the same as successful transactions and the numbers all seem sane. That's the kind of thing manual intervention often picks up; we're used to seeing users enter things incorrectly, with no care. That wasn't the case here, so where do those come from, anyway? Now we've got logs and line numbers, we found the following snippets:

{% gist icbat/ed53c7ac698ff714726b %}

Negative dates don't make much sense, but in the context of the code, this'll get you pretty close. But this should fail every single time, right? It should've been failing since this code was written! No code changes were made!

A six hour difference is a very big red flag where I'm from, though. This team struggles with this all the time. One of our systems is in UTC, the other is in local time, which is UTC-6. _Well, butts_. We ran a quick `date` command on each system (BS and Node1), and they were indeed in different time zones. 
